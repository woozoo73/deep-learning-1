{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서플로우(TensorFlow) 시작하기\n",
    "[원본 문서](https://www.tensorflow.org/get_started/get_started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 안내서는 텐서플로우에서 프로그래밍을 시작하도록 안내합니다. 이 안내서를 사용하기 전에, [텐서플로우를 설치하십시오](https://www.tensorflow.org/install/). 이 가이드를 최대한 활용하려면, 다음 사항들을 알아야 합니다:\n",
    "\n",
    "* 파이썬(Python)으로 프로그래밍하는 법.\n",
    "* 적어도 배열에 대한 약간의 지식.\n",
    "* 이상적으로는, 기계 학습에 관한 것입니다. 그러나, 기계 학습에 대해 거의 또는 전혀 알지 못하는 경우에도, 이것이 그래도 읽어야 할 첫 번째 가이드입니다.\n",
    "\n",
    "텐서플로우는 여러 API들을 제공합니다. 최저 수준의 API--텐서플로우 Core--는 완벽한 프로그래밍 제어를 제공합니다. 텐서플로우 Core는 기계 학습 연구자 및 모델을 정밀하게 제어해야 하는 사람들을 위해 권장됩니다. 높은 수준의 API는 텐서플로우 Core 위에 구축됩니다. 이러한 상위 수준의 API는 일반적으로 텐서플로우 Core보다 배우고 사용하기가 쉽습니다. 또한 상위 수준의 API는 반복적인 작업을 여러 사용자간에 보다 쉽고 일관되게 만듭니다. tf.estimator와 같은 고급 API를 통해 데이터 셋, 추정자(estimator), 학습 및 추론을 처리할 수 있습니다.\n",
    "\n",
    "이 안내서는 텐서플로우 Core에 대한 자습서로 시작됩니다. 나중에, tf.estimator에서 동일한 모델을 구현하는 방법을 보여줍니다. 텐서플로우 Core의 원리를 아는 것은 보다 간결한 고수준의 API를 사용할 때 내부적으로 어떤 일이 일어나는 지에 대한 훌륭한 개념 모델을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텐서(Tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우 데이터의 중심 단위는 **텐서(tensor)**입니다. 텐서는 임의의 차원 배열 구조(shape)인 원시 값들(primitive values)의 집합으로 구성됩니다. 텐서의 **순위(rank)**는 차원 수입니다. 다음은 텐서들의 몇 가지 예입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 # a rank 0 tensor; this is a scalar with shape []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1., 2., 3.] # a rank 1 tensor; this is a vector with shape [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서플로우 핵심 자습서\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서플로우 가져 오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우 프로그램에 대한 표준 import 문은 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 하면 파이썬은 텐서플로우의 모든 클래스, 메소드 및 심볼에 접근 할 수 있습니다. 대부분의 문서에서는 이미 이 작업을 했다고 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연산 그래프(Computational Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우 핵심 프로그램은 두 개의 분리된 부분으로 구성되어 있다고 생각할 수 있습니다:\n",
    "\n",
    "1. 연산 그래프의 작성.\n",
    "2. 연산 그래프의 실행.\n",
    "\n",
    "**연산 그래프**는 일련의 텐서플로우 작업을 노드 그래프로 배열한 것입니다. 간단한 연산 그래프를 작성해 봅시다. 각 노드는 0 이상의 텐서를 입력으로 사용하고 텐서를 출력으로 생성합니다. 노드의 한 유형은 상수입니다. 모든 텐서플로우 상수와 마찬가지로, 입력을 받지 않으며, 내부적으로 저장하는 값을 출력합니다. 다음과 같이 두 개의 부동 소수점 텐서 `node1`과 `node2`를 생성할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 출력문의 결과는 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노드의 출력이 아마도 기대했을 값들인 `3.0`과 `4.0`로 출력되지 않는다는 것에 주목하십시오. 대신, 평가될 때, 각각 3.0과 4.0를 생성하는 노드입니다. 노드를 실제로 평가하려면, **세션(session)** 내에서 연산 그래프를 실행해야 합니다. 세션은 텐서플로우 런타임의 제어(control)와 상태(state)를 캡슐화합니다.\n",
    "\n",
    "다음 코드는 `Session` 객체를 만든 다음 `run` 메소드를 호출하여 `node1`과 `node2`를 평가하는 만큼의 연산 그래프를 실행합니다. 다음과 같이 세션에서 연산 그래프를 실행하면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예상했던 값인 3.0과 4.0가 나타납니다: ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensor` 노드들을 작업(작업 또한 노드입니다)들과 결합하여 보다 복잡한 연산을 만들 수 있습니다. 예를 들어, 두 개의 상수 노드를 추가하여 다음과 같이 새 그래프를 생성 할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3:\", node3)\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 두 개의 출력 문의 결과입니다 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우는 연산 그래프의 그림을 표시하는 텐서보드(TensorBoard)라는 유틸리티를 제공합니다. 다음은 텐서보드가 그래프를 시각화하는 방법을 보여주는 스크린 샷입니다:\n",
    "<img alt=\"TensorBoard screenshot\" src=\"https://www.tensorflow.org/images/getting_started_add.png\">\n",
    "이대로, 이 그래프는 항상 동일한 결과를 산출하기 때문에 특별히 흥미롭지는 않습니다. **자리 표시자(placeholder)**라고 하는 외부 입력을 허용하도록 그래프를 매개변수화(parameterized)할 수 있습니다. **자리 표시자**는 나중에 값을 제공하겠다는 약속입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 세 줄은 입력 매개변수들(a와 b)을 정의한 다음 해당 매개변수들에 대한 연산을 정의하는 함수 또는 람다와 조금 비슷합니다. [run 메소드](https://www.tensorflow.org/api_docs/python/tf/Session#run)에 feed_dict 인수를 통해 구체적인 값을 자리표시자에 공급함으로써 이 그래프를 여러 입력으로 평가할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 결과 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서보드(TensorBoard)에서 그래프는 다음과 같습니다:\n",
    "<img alt=\"TensorBoard screenshot\" src=\"https://www.tensorflow.org/images/getting_started_adder.png\">\n",
    "다른 연산을 추가하여 연산 그래프를 더 복잡하게 만들 수 있습니다. 예를 들어,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b: 4.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력을 만듭니다 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞의 연산 그래프는 텐서보드에서 다음과 같이 나타납니다:\n",
    "<img alt=\"TensorBoard screenshot\" src=\"https://www.tensorflow.org/images/getting_started_triple.png\">\n",
    "기계 학습에서 우리는 일반적으로 위와 같은 임의의 입력을 취할 수 있는 모델을 원할 것입니다. 모델을 학습 가능하게 만들려면, 동일한 입력으로 새로운 출력을 얻기 위해 그래프를 수정할 수 있어야 합니다. **변수**를 사용하면 그래프에 학습 가능한 매개변수를 추가할 수 있습니다. 그것들은 타입과 초기 값으로 구성됩니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상수는 `tf.constant`를 호출할 때 초기화되며 값은 절대로 변경될 수 없습니다. 반대로 `tf.Variable`을 호출하면 변수가 초기화되지 않습니다. 텐서플로우 프로그램의 모든 변수를 초기화하려면 다음과 같이 특수한 작업을 명시적으로 호출해야합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`init`이 텐서플로우 하위 그래프에 모든 전역 변수를 초기화하는 처리임을 인식하는 것이 중요합니다. `sess.run`을 호출할 때까지 변수는 초기화되지 않습니다.\n",
    "\n",
    "`x`는 자리표시자이므로, 다음과 같이 `x`의 여러 값들에 대해 `linear_model`을 일제히 평가할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x: [1, 2, 3, 4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력을 만듭니다 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 모델을 만들었지만, 아직 얼마나 좋은지 모릅니다. 교육 데이터에 대한 모델을 평가하려면, 원하는 값을 제공하기 위해 `y` 자리표시자가 필요하며, 손실함수(loss function)를 작성해야 합니다.\n",
    "\n",
    "손실함수는 제공된 모델로부터 현재 모델이 얼마나 떨어져 있는지를 측정합니다. 현재 모델과 제공된 데이터 사이의 델타 제곱을 합한 선형회귀분석(linear regression)에 표준손실모델(standard loss model)을 사용합니다. `linear_model - y`는 각 요소에 해당하는 예의 오차 델타 벡터를 만듭니다. `tf.square`를 호출하여 오차를 제곱합니다. 그런 다음, `tf.reduce_sum`을 사용해, 모든 오차의 제곱을 합함으로써 모든 예들의 오차를 대표(abstract)하는 단일 스칼라를 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실 값을 만듭니다 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W`와 `b`를 정확한 값들인 -1과 1로 재할당하여 수동으로 향상시킬 수 있습니다. 변수는 `tf.Variable`을 통해 초기화되지만 `tf.assign`과 같은 연산을 사용해 변경할 수 있습니다. 예를 들어, `W=-1`과 `b=1`은 우리 모델에서 최적의 매개변수들입니다. `W`와 `b`를 적절하게 변경할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 출력은 손실이 0임을 보여줍니다. ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 `W`와 `b`의 \"정확한\" 값들을 추측했지만, 기계 학습의 요점은 올바른 모델 매개변수들을 자동으로 찾는 것입니다. 다음 절에서 이것을 이루는 방법을 보여주겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.train API\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계 학습에 대한 완전한 논의는 본 자습서의 범위를 벗어납니다. 하지만, 텐서플로우는 손실 함수를 최소화하기 위해 각 변수를 조금씩 변경하는 **옵티마이저(optimizer)**들을 제공합니다. 가장 간단한 옵티마이저는 **경사 하강(gradient descent)**입니다. 해당 변수의 손실에 대한 미분 크기에 따라 각 변수를 수정합니다. 일반적으로, 수작업으로 계산하는 것은 지루하고 오류가 발생하기 쉽습니다. 따라서, 텐서플로우는 `tf.gradients` 함수를 사용하여 주어진 모델에 대한 서술만으로 미분값을 자동으로 생성할 수 있습니다. 단순화 하자면, 옵티마이저가 일반적으로 이를 수행합니다. 예를 들어,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "\n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 모델에서 매개변수 결과: ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 진짜 기계 학습을 했습니다! 이 간단한 선형회귀를 수행하더라도 텐서플로우 코어 코드가 많이 필요하지는 않지만, 모델에 데이터를 입력하는 더 복잡한 모델과 메서드는 더 많은 코드를 필요로 합니다. 그래서 텐서플로우는 일반적인 패턴, 구조 및 기능에 대한 보다 높은 수준의 추상화를 제공합니다. 다음 절에서 이러한 추상화의 일부를 사용하는 방법을 배웁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 완성된 프로그램"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "완성된 훈련 가능한 선형회귀 모델을 보면:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행하면, 결과가 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실이 아주 작은 수(거의 0에 가깝습니다)라는 것에 주목하세요. 이 프로그램을 실행하면 모델이 의사 난수 값으로 초기화되기 때문에 손실이 정확히 같지 않을 수 있습니다.\n",
    "\n",
    "이런 보다 복잡한 프로그램 역시 텐서보드에서 시각화할 수 있습니다.\n",
    "<img alt=\"TensorBoard final model visualization\" src=\"https://www.tensorflow.org/images/getting_started_final.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.estimator\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.estimator`는 기계 학습의 기법을 단순화하는 고수준 텐서플로우 라이브러리로, 다음을 포함합니다:\n",
    "\n",
    "* 학습 루프의 실행\n",
    "* 평가 루프의 실행\n",
    "* 데이터셋 관리\n",
    "\n",
    "tf.estimator는 많은 일반 모델들을 정의합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 사용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.estimator`로 선형회귀 프로그램이 얼마나 간단해졌는지 주목하십시오:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp1ovxbcgn\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp1ovxbcgn', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp1ovxbcgn/model.ckpt.\n",
      "INFO:tensorflow:loss = 13.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1253.18\n",
      "INFO:tensorflow:loss = 0.404879, step = 101 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1367.07\n",
      "INFO:tensorflow:loss = 0.0710876, step = 201 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1332.47\n",
      "INFO:tensorflow:loss = 0.0199711, step = 301 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1223.98\n",
      "INFO:tensorflow:loss = 0.00589484, step = 401 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1282.52\n",
      "INFO:tensorflow:loss = 0.000747879, step = 501 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1288.54\n",
      "INFO:tensorflow:loss = 0.000365402, step = 601 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1324.07\n",
      "INFO:tensorflow:loss = 9.57571e-05, step = 701 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1344.97\n",
      "INFO:tensorflow:loss = 1.66231e-05, step = 801 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1292.06\n",
      "INFO:tensorflow:loss = 2.05724e-06, step = 901 (0.077 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp1ovxbcgn/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.19623e-06.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-25-08:16:49\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp1ovxbcgn/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-25-08:16:49\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 2.30808e-07, global_step = 1000, loss = 9.23232e-07\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-25-08:16:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp1ovxbcgn/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-25-08:16:50\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00256808, global_step = 1000, loss = 0.0102723\n",
      "train metrics: {'average_loss': 2.3080794e-07, 'loss': 9.2323177e-07, 'global_step': 1000}\n",
      "eval metrics: {'average_loss': 0.0025680761, 'loss': 0.010272305, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "\n",
    "# Declare list of features. We only have one numeric feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# linear classification, and many neural network classifiers and regressors.\n",
    "# The following code provides an estimator that does linear regression.\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set.\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행하면, 결과가 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 평가 데이터가 얼마간 더 큰 손실이 났지만, 여전히 0에 가깝다는 것에 주목하세요. 그것은 우리가 올바르게 학습하고 있음을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 커스텀 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.estimator`는 미리 정의된 모델들로만 제한하지 않습니다. 텐서플로우에 내장되어 있지 않은 커스텀 모델을 만들고 싶다고 가정해 보겠습니다. 우리는 여전히`tf.estimator`의 데이터셋, 공급(feeding), 훈련 등에 대한 고수준 추상화를 유지할 수 있습니다. 예를 들어, 하위 레벨 텐서플로우 API에 대한 지식을 사용하여 `LinearRegressor`와 동등한 우리만의 모델을 구현하는 방법을 보여주겠습니다.\n",
    "\n",
    "`tf.estimator`와 함께 동작하는 커스텀 모델을 정의하려면, `tf.estimator.Estimator`를 사용해야 합니다. `tf.estimator.LinearRegressor`는 실제로 `tf.estimator.Estimator`의 하위 클래스입니다. `Estimator`의 하위 클래스로 하는 대신에, 단순히 `tf.estimator`에게 어떻게 예측, 학습 단계, 손실을 평가할 수 있는지를 알려주는 `model_fn` 함수를 `Estimator`에게 제공합니다. 코드는 다음과 같습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpwns0ghnb\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpwns0ghnb', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpwns0ghnb/model.ckpt.\n",
      "INFO:tensorflow:loss = 17.3073748566, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1339.49\n",
      "INFO:tensorflow:loss = 0.0416367058214, step = 101 (0.078 sec)\n",
      "INFO:tensorflow:global_step/sec: 1318.66\n",
      "INFO:tensorflow:loss = 0.00252774516377, step = 201 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1363.17\n",
      "INFO:tensorflow:loss = 5.93164722352e-05, step = 301 (0.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 1441.82\n",
      "INFO:tensorflow:loss = 1.58568519768e-05, step = 401 (0.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 1343.92\n",
      "INFO:tensorflow:loss = 4.48332115362e-07, step = 501 (0.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 1270.99\n",
      "INFO:tensorflow:loss = 1.33004764392e-07, step = 601 (0.076 sec)\n",
      "INFO:tensorflow:global_step/sec: 1366.29\n",
      "INFO:tensorflow:loss = 8.78050233993e-09, step = 701 (0.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 1300.2\n",
      "INFO:tensorflow:loss = 1.66211022091e-09, step = 801 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1316.22\n",
      "INFO:tensorflow:loss = 1.24108052492e-10, step = 901 (0.076 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmpwns0ghnb/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.7129808447e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-25-08:20:46\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpwns0ghnb/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-25-08:20:46\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1.26799e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-25-08:20:46\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpwns0ghnb/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-25-08:20:47\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101007\n",
      "train metrics: {'loss': 1.2679883e-11, 'global_step': 1000}\n",
      "eval metrics: {'loss': 0.010100667, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features, we only have one real-valued feature\n",
    "def model_fn(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W * features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # EstimatorSpec connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# train\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행하면, 결과는 ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커스텀 `model_fn()` 함수의 내용이 저수준 API의 수동 모델 훈련 루프와 얼마나 비슷한지 주목하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 단계\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 텐서플로우의 기본에 대한 실무 지식을 습득했습니다. 우리는 더 많이 배울 수있는 몇 가지 자습서들을 가지고 있습니다. 기계 학습의 초보자인 경우는 [MNIST for beginners](https://www.tensorflow.org/get_started/mnist/beginners)를 보시고, 그렇지 않으면 [Deep MNIST for experts](https://www.tensorflow.org/get_started/mnist/pros)를 보세요.\n",
    "<hr/>\n",
    "별도로 명시된 경우를 제외하고, 이 페이지의 내용은 Creative Commons Attribution 3.0 License로 라이센스가 부여되며, 코드 샘플은 Apache 2.0 License에 의거한 라이센스가 부여됩니다. 자세한 내용은 우리의 사이트 정책을 참조하십시오. Java는 Oracle 및/또는 그 계열사의 등록 상표입니다.\n",
    "\n",
    "2017.8월 17, 최종 수정됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
